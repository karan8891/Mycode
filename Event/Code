This is a complete end-to-end implementation in Python + Airflow DAG + Terraform concepts.

It simulates Five9 (APMF) uploading files and CDMP (GDW) receiving the transfer.

The STS job is dynamically created and deleted. It waits for STS completion before cleanup and notifying CDMNxt.

------------------------------

1. Cloud Function (APMF side)

------------------------------

Triggered by Five9's Pub/Sub with schema: bucket, object, delete_after_transfer

def main(event, context): import base64, json, requests

if 'data' not in event:
    raise ValueError("No data in Pub/Sub message")

message = json.loads(base64.b64decode(event['data']).decode('utf-8'))

dag_payload = {
    "bucket": message.get("bucket"),
    "object": message.get("object"),
    "project_id": message.get("project_id"),
    "delete_after_transfer": message.get("delete_after_transfer", False)
}

dag_trigger_url = "https://<composer-webserver-url>/api/experimental/dags/event_sts_transfer_dag/dag_runs"
headers = {"Content-Type": "application/json", "Authorization": "Bearer <ID_TOKEN>"}

requests.post(dag_trigger_url, headers=headers, json={"conf": dag_payload})

------------------------------

2. Airflow DAG (CDMP side)

------------------------------

from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.google.cloud.operators.cloud_storage_transfer_service import ( CloudDataTransferServiceCreateJobOperator, CloudDataTransferServiceRunJobOperator, CloudDataTransferServiceDeleteJobOperator, ) from airflow.providers.google.cloud.sensors.pubsub import PubSubPullSensor from airflow.utils.dates import days_ago import json, uuid

def create_sts_job_payload(conf): job_name = f"transferJobs/{str(uuid.uuid4())[:8]}" return { "description": job_name, "project_id": "cdmp-project-id", "transfer_spec": { "gcs_data_source": {"bucket_name": conf["bucket"]}, "gcs_data_sink": {"bucket_name": "cdmp-destination-bucket"}, "object_conditions": {"include_prefixes": [conf["object"]]} }, "status": "ENABLED", "notification_config": { "pubsub_topic": "projects/cdmp-project/topics/sts-completion-topic", "event_types": ["TRANSFER_OPERATION_SUCCESS"] }, "transfer_options": { "delete_objects_from_source_after_transfer": conf.get("delete_after_transfer", False) } }

with DAG( "event_sts_transfer_dag", schedule_interval=None, start_date=days_ago(1), catchup=False, params={"job_id": None} ) as dag:

def build_transfer_body(**kwargs):
    conf = kwargs['dag_run'].conf
    return create_sts_job_payload(conf)

create_job = CloudDataTransferServiceCreateJobOperator(
    task_id="create_sts_job",
    body=build_transfer_body(),
    do_xcom_push=True
)

run_job = CloudDataTransferServiceRunJobOperator(
    task_id="run_sts_job",
    job_name="{{ task_instance.xcom_pull(task_ids='create_sts_job')['description'] }}",
    project_id="cdmp-project-id"
)

wait_for_completion = PubSubPullSensor(
    task_id="wait_for_completion",
    project_id="cdmp-project-id",
    subscription="projects/cdmp-project/subscriptions/sts-completion-sub",
    ack_messages=True,
    max_messages=5,
    timeout=600
)

def delete_job_and_notify(**kwargs):
    import requests
    job_id = kwargs['ti'].xcom_pull(task_ids='create_sts_job')['description']
    # Notify downstream service (CDMNxt) with TOC file name
    requests.post("https://cdmnxt-service-url.com/trigger", json={"toc_file": kwargs['dag_run'].conf['object']})
    return job_id

cleanup = PythonOperator(
    task_id="delete_job_notify",
    python_callable=delete_job_and_notify
)

delete_job = CloudDataTransferServiceDeleteJobOperator(
    task_id="delete_sts_job",
    job_name="{{ task_instance.xcom_pull(task_ids='create_sts_job')['description'] }}",
    project_id="cdmp-project-id"
)

create_job >> run_job >> wait_for_completion >> cleanup >> delete_job

------------------------------

3. Terraform (example blocks)

------------------------------

In cloudfunction module (main.tf):

resource "google_cloudfunctions_function" "event_trigger" { name        = "five9-cf-trigger" runtime     = "python39" entry_point = "main" source_archive_bucket = "apmf-cloud-function-src" source_archive_object = "src.zip" event_trigger { event_type = "google.pubsub.topic.publish" resource   = google_pubsub_topic.five9_topic.id } }

In pubsub module:

resource "google_pubsub_topic" "five9_topic" { name = "five9-trigger-topic" } resource "google_pubsub_topic" "completion_topic" { name = "sts-completion-topic" } resource "google_pubsub_subscription" "completion_sub" { name  = "sts-completion-sub" topic = google_pubsub_topic.completion_topic.id }

------------------------------

Summary:

- Five9 sends upload event via Pub/Sub

- APMF CF triggers Airflow DAG

- DAG creates/runs/deletes STS job

- Waits for STS completion

- Notifies CDMNxt via HTTP (or triggers next DAG/CF)

