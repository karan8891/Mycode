from airflow import models
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.utils.dates import days_ago

BUCKET_NAME = "gdw1_sandbox-corp-gdw-sfr-cdb8"
SOURCE_PREFIX = "gb_files/"
DESTINATION_FILE = "gb_files/merged/merged_output.txt"

def concatenate_files_gcs(bucket_name, source_prefix, destination_blob):
    hook = GCSHook()
    client = hook.get_conn()
    bucket = client.bucket(bucket_name)

    # List all blobs with the prefix
    blobs = list(client.list_blobs(bucket_name, prefix=source_prefix))
    contents = []

    for blob in blobs:
        if blob.name.endswith(".txt") and not blob.name.endswith(destination_blob):
            content = blob.download_as_bytes()
            contents.append(content)

    # Write to destination blob
    final_blob = bucket.blob(destination_blob)
    final_blob.upload_from_string(b"\n".join(contents))

with models.DAG(
    dag_id="concat_gcs_text_files",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["gcs", "merge"],
) as dag:

    concat_task = PythonOperator(
        task_id="concatenate_files",
        python_callable=concatenate_files_gcs,
        op_kwargs={
            "bucket_name": BUCKET_NAME,
            "source_prefix": SOURCE_PREFIX,
            "destination_blob": DESTINATION_FILE,
        },
    )

    concat_task
